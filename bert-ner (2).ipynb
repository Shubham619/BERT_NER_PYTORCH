import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from datasets import load_dataset

class LLMWithVectorDB:
    def __init__(self, embedding_model_name, gpt_model_name, index_file, embedding_dim=768):
        self.embedding_model = SentenceTransformer(embedding_model_name)
        self.tokenizer = GPT2Tokenizer.from_pretrained(gpt_model_name)
        self.gpt_model = GPT2LMHeadModel.from_pretrained(gpt_model_name)
        self.index_file = index_file
        self.embedding_dim = embedding_dim

        # Load or create the Faiss index
        if index_file and os.path.exists(index_file):
            self.index = faiss.read_index(index_file)
            print("Loaded Faiss index from file.")
        else:
            self.index = faiss.IndexFlatL2(self.embedding_dim)
            print("Created new Faiss index.")

        # Load dataset (Natural Questions from Google)
        self.dataset = load_dataset("natural_questions")

    def get_gpt2_answer(self, prompt, context=""):
        # Combine the context (from previous answers) with the current query
        if context:
            prompt = f"{context}\n\n{prompt}"

        inputs = self.tokenizer.encode(prompt, return_tensors="pt")
        outputs = self.gpt_model.generate(inputs, max_length=150, num_return_sequences=1)
        return self.tokenizer.decode(outputs[0], skip_special_tokens=True)

    def add_question_to_vectordb(self, questions):
        # Convert questions to embeddings
        embeddings = self.embedding_model.encode(questions)

        # Normalize the embeddings
        faiss.normalize_L2(embeddings)

        # Add embeddings to the Faiss index
        self.index.add(np.array(embeddings, dtype=np.float32))

        # Save the updated index
        if self.index_file:
            faiss.write_index(self.index, self.index_file)

    def search_vectordb(self, query, k=5):
        # Convert the query into an embedding
        query_embedding = self.embedding_model.encode([query])

        # Normalize the query embedding
        faiss.normalize_L2(query_embedding)

        # Search for top-k most similar questions
        distances, indices = self.index.search(np.array(query_embedding, dtype=np.float32), k)

        return distances, indices

    def get_answer(self, query, threshold=0.7):
        distances, indices = self.search_vectordb(query)

        # Filter out relevant results based on threshold
        relevant_indices = [idx for dist, idx in zip(distances[0], indices[0]) if dist < threshold]
        
        if relevant_indices:
            # If relevant questions are found, combine their answers
            combined_answer = " ".join([self.dataset["train"][idx]["answer"] for idx in relevant_indices])
            print(f"Combining answers from VectorDB (distances below threshold): {combined_answer}")
            
            # Pass the combined answer as context to GPT-2
            gpt_answer = self.get_gpt2_answer(query, context=combined_answer)
            return gpt_answer
        else:
            # If no relevant questions found, query GPT-2 Large
            print(f"No relevant question found, querying GPT-2 Large")
            gpt_answer = self.get_gpt2_answer(query)
            
            # Store this query in the vector database for future reference
            self.add_question_to_vectordb([query])
            return gpt_answer

# Usage Example

if __name__ == "__main__":
    vectordb = LLMWithVectorDB(
        embedding_model_name="sentence-transformers/multi-qa-mpnet-base-dot-v1",
        gpt_model_name="gpt2-large",
        index_file="vectordb.index",
        embedding_dim=768
    )

    # Example usage: answer a batch of questions
    user_questions = [
        "What is the capital of France?",
        "Who is the president of the United States?",
        "How does a neural network work?",
        "What is the square root of 64?"
    ]

    for question in user_questions:
        answer = vectordb.get_answer(question)
        print(f"Question: {question}\nAnswer: {answer}\n")

    # You can also load and use the Natural Questions dataset:
    dataset = vectordb.dataset["train"]
    print(f"First example from Natural Questions dataset: {dataset[0]['question']}")
